### **Phase 1: Core Framework Setup**

**Goal**: Pygame + Flask + WebSocket base environment

1. âœ… Create Pygame window with fixed-size map (3 zones: garden, hall, vault)
2. âœ… Place 3 named NPCs with avatars
3. âœ… Set up Flask REST server and Flask-SocketIO for bidirectional comm
4. âœ… Pygame sends:
    - Agent positions
    - Tick updates
5. âœ… Server sends:
    - Actions to perform
    - Dialogues to display
6. âœ… Test:
    - Move NPC from A to B
    - Print JSON response in console
    - Trigger dummy dialogue
7. âœ… Log output: All actions + dialogues


### WORKING ON PHASE 1
	1.	Set up a modular FastAPI backend with REST + WebSocket support.
	2.	Pygame visualizes 3 fixed-position NPCs on a grid.
	3.	Agents move when server sends a command (walk to X).
	4.	Pygame emits real-time position updates over WebSocket.
	5.	All components (backend, state, comms) are cleanly decoupled for future scaling.


ğŸ¯ PHASE 2 GOAL

Give each agent a modular internal brain that includes:

	â€¢	âœ… A memory stream (observations, dialogues, reflections)
	â€¢	âœ… A belief table (trust scores for other agents)
	â€¢	âœ… An emotion vector (quantified feelings toward events)

Weâ€™ll also:
	â€¢	Expose this via REST APIs (e.g., GET /agent/John/state)
	â€¢	Send dummy observations to test belief/emotion/memory updates
	â€¢	Lay the foundation for reflection + planning in future phases



ğŸ§  PHASE 3: Dialogue + Event Logging + Summary

â¸»

ğŸ¯ Goal
	â€¢	Enable agents to talk via LLM-generated dialogue.
	â€¢	Log all dialogue lines + create a named transcript file (e.g., dialog_john_anna).
	â€¢	After conversation ends, each agent generates a personal summary of the interaction.
	â€¢	Store that summary in their observation memory using log_observation.

ğŸ“¦ WHAT WEâ€™LL BUILD

âœ… Dialogue Manager (dialogue_manager.py)
	â€¢	Stores ongoing dialogue
	â€¢	Logs every line with timestamp
	â€¢	Ends dialogue, returns full history

âœ… New REST endpoints
	â€¢	POST /dialogue/start
	â€¢	POST /dialogue/speak
	â€¢	POST /dialogue/end

âœ… Summary generation using Ollama (LLM)
	â€¢	Prompt LLM to summarize a dialogue from one agentâ€™s perspective
	â€¢	Add to their memory as MemoryItem

PHASE 3 Summary:
âœ… Memory Structure Breakdown:
	â€¢	Agent: John
	â€¢	Memory: Includes a personalized LLM summary of the dialogue
	â€¢	Belief scores: Empty for now (weâ€™ll use this in Phase 4)
	â€¢	Emotion vector: Untouched so far (also Phase 4+)


PHASE 4 SUMMARY

âœ… What Weâ€™ll Build
	1.	ğŸ”§ Add plan field to AgentState
	2.	âœï¸ Create LLM-powered daily planner
	3.	ğŸ”„ Add reaction override logic (based on memory/emotion/belief)
	4.	ğŸŒ REST APIs to:
	â€¢	Get current plan
	â€¢	Generate a new plan
	â€¢	React to an observation


PHASE 5 GOAL

Give each agent the ability to reflect periodically on their past events and generate high-level insights, and to retrieve relevant memories using a Recency, Relevance, and Importance (BERRI) weighted scoring system.

â¸»

âœ… What Weâ€™ll Do
	1.	Add FAISS to index agent memory embeddings.
	2.	Create a RAG (Retrieval-Augmented Generation) system to fetch top-K relevant memories using cosine similarity.
	3.	Implement Reflection Trigger Logic:
	â€¢	If recent memories cross importance threshold, summarize them via LLM.
	4.	Add REST endpoints:
	â€¢	GET /agent/{name}/retrieve?q=...
	â€¢	POST /agent/{name}/reflect



Phase 6 â€” Scripted Scenario + Deduction Loop

This is where you bring all systems together for a full simulation cycle:

Youâ€™ll finally see:
	â€¢	ğŸ§  Agents make a plan
	â€¢	ğŸ—£ï¸ Talk and remember
	â€¢	ğŸ¤¨ Reflect on what happened
	â€¢	ğŸ¤¯ React if things seem off
	â€¢	ğŸ§¾ Build suspicion through memory

âœ… What Weâ€™ll Build
	1.	Add an API: POST /scenario/test1
â†’ Inject 3â€“4 scripted memories (some true, some false)
	2.	Let agents talk using dialogue API
	3.	Trigger /reflect + /react to see deduction happen
	4.	Final: use /state to see if belief/emotion updated