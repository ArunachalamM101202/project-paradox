### **Phase 1: Core Framework Setup**

**Goal**: Pygame + Flask + WebSocket base environment

1. âœ… Create Pygame window with fixed-size map (3 zones: garden, hall, vault)
2. âœ… Place 3 named NPCs with avatars
3. âœ… Set up Flask REST server and Flask-SocketIO for bidirectional comm
4. âœ… Pygame sends:
    - Agent positions
    - Tick updates
5. âœ… Server sends:
    - Actions to perform
    - Dialogues to display
6. âœ… Test:
    - Move NPC from A to B
    - Print JSON response in console
    - Trigger dummy dialogue
7. âœ… Log output: All actions + dialogues


### WORKING ON PHASE 1
	1.	Set up a modular FastAPI backend with REST + WebSocket support.
	2.	Pygame visualizes 3 fixed-position NPCs on a grid.
	3.	Agents move when server sends a command (walk to X).
	4.	Pygame emits real-time position updates over WebSocket.
	5.	All components (backend, state, comms) are cleanly decoupled for future scaling.


ğŸ¯ PHASE 2 GOAL

Give each agent a modular internal brain that includes:

	â€¢	âœ… A memory stream (observations, dialogues, reflections)
	â€¢	âœ… A belief table (trust scores for other agents)
	â€¢	âœ… An emotion vector (quantified feelings toward events)

Weâ€™ll also:
	â€¢	Expose this via REST APIs (e.g., GET /agent/John/state)
	â€¢	Send dummy observations to test belief/emotion/memory updates
	â€¢	Lay the foundation for reflection + planning in future phases



ğŸ§  PHASE 3: Dialogue + Event Logging + Summary

â¸»

ğŸ¯ Goal
	â€¢	Enable agents to talk via LLM-generated dialogue.
	â€¢	Log all dialogue lines + create a named transcript file (e.g., dialog_john_anna).
	â€¢	After conversation ends, each agent generates a personal summary of the interaction.
	â€¢	Store that summary in their observation memory using log_observation.

ğŸ“¦ WHAT WEâ€™LL BUILD

âœ… Dialogue Manager (dialogue_manager.py)
	â€¢	Stores ongoing dialogue
	â€¢	Logs every line with timestamp
	â€¢	Ends dialogue, returns full history

âœ… New REST endpoints
	â€¢	POST /dialogue/start
	â€¢	POST /dialogue/speak
	â€¢	POST /dialogue/end

âœ… Summary generation using Ollama (LLM)
	â€¢	Prompt LLM to summarize a dialogue from one agentâ€™s perspective
	â€¢	Add to their memory as MemoryItem

PHASE 3 Summary:
âœ… Memory Structure Breakdown:
	â€¢	Agent: John
	â€¢	Memory: Includes a personalized LLM summary of the dialogue
	â€¢	Belief scores: Empty for now (weâ€™ll use this in Phase 4)
	â€¢	Emotion vector: Untouched so far (also Phase 4+)


PHASE 4 SUMMARY

âœ… What Weâ€™ll Build
	1.	ğŸ”§ Add plan field to AgentState
	2.	âœï¸ Create LLM-powered daily planner
	3.	ğŸ”„ Add reaction override logic (based on memory/emotion/belief)
	4.	ğŸŒ REST APIs to:
	â€¢	Get current plan
	â€¢	Generate a new plan
	â€¢	React to an observation


PHASE 5 GOAL

Give each agent the ability to reflect periodically on their past events and generate high-level insights, and to retrieve relevant memories using a Recency, Relevance, and Importance (BERRI) weighted scoring system.

â¸»

âœ… What Weâ€™ll Do
	1.	Add FAISS to index agent memory embeddings.
	2.	Create a RAG (Retrieval-Augmented Generation) system to fetch top-K relevant memories using cosine similarity.
	3.	Implement Reflection Trigger Logic:
	â€¢	If recent memories cross importance threshold, summarize them via LLM.
	4.	Add REST endpoints:
	â€¢	GET /agent/{name}/retrieve?q=...
	â€¢	POST /agent/{name}/reflect



Phase 6 â€” Scripted Scenario + Deduction Loop

This is where you bring all systems together for a full simulation cycle:

Youâ€™ll finally see:
	â€¢	ğŸ§  Agents make a plan
	â€¢	ğŸ—£ï¸ Talk and remember
	â€¢	ğŸ¤¨ Reflect on what happened
	â€¢	ğŸ¤¯ React if things seem off
	â€¢	ğŸ§¾ Build suspicion through memory

âœ… What Weâ€™ll Build
	1.	Add an API: POST /scenario/test1
â†’ Inject 3â€“4 scripted memories (some true, some false)
	2.	Let agents talk using dialogue API
	3.	Trigger /reflect + /react to see deduction happen
	4.	Final: use /state to see if belief/emotion updated

ğŸ§  Phase 7: Belief Scoring + Emotion Feedback Loop

â¸»

ğŸ¯ GOAL

Create a modular, score-based system that lets agents:

	â€¢	Adjust belief (trust) in other agents
	â€¢	Adjust emotions like suspicion, stress, anger
	â€¢	Trigger these updates from memory events and dialogue

â¸»

âœ… What Weâ€™ll Build

ğŸ”¸ 1. Emotion + Belief Hooks

When a memory is logged (via /observe or /dialogue), we:
	â€¢	Analyze the content
	â€¢	Adjust the agentâ€™s belief_scores
	â€¢	Adjust their emotion_vector

ğŸ”¸ 2. Add REST API:
	â€¢	/agent/{name}/update-belief-emotion

Processes existing memory and updates state

ğŸ”¸ 3. Add LLM hook (optional override)

Ask: â€œHow should John feel about Anna based on this?â€

â¸»

ğŸ§± Update Strategy (Lightweight & Modular)

To keep things flexible:
	â€¢	All emotion + belief updates go into a single helper file:
belief_emotion_engine.py
	â€¢	This file is auto-called whenever an observe or dialogue_end happens


 Phase 8: Memory Compression + RAG (FAISS) Integration

â¸»

ğŸ¯ GOAL

Agents store tons of events over time. We now:

	â€¢	Offload old memory into compressed summaries
	â€¢	Store embeddings via FAISS
	â€¢	Retrieve contextually relevant memories for planning, reacting, etc.

This mimics:
	â€¢	ğŸ’¾ Short-term memory â†’ agent.memory (live)
	â€¢	ğŸ“š Long-term memory â†’ summarized + vectorized via FAISS
	â€¢	ğŸ§  Use both during LLM prompts for plan, react, reflect


âœ… What Weâ€™ll Build
	1.	ğŸ” Move old memory (>N) into summary
	2.	ğŸ§  Run LLM compression: summarize 3â€“5 logs into 1 compressed item
	3.	ğŸ’¾ Store compressed text + FAISS embedding
	4.	ğŸ” Add retrieve_memories(query) â†’ RAG from compressed memory
	5.	ğŸ”Œ Update /plan, /react, /reflect to optionally include top-K relevant past summaries